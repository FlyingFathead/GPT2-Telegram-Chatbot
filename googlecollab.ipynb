{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMB9+ldyWlqlu6sTXtPOSy7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/Existencce/d6fd106e6d3d799c2be0da480541b9d1/untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kt5roV6vPrKa",
        "outputId": "1e5be868-001d-4be2-e958-4a7c903c989f"
      },
      "source": [
        "!git clone https://github.com/paper2code/GPT2-Telegram-Chatbot /content/gpt2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '/content/gpt2'...\n",
            "remote: Enumerating objects: 554, done.\u001b[K\n",
            "remote: Counting objects: 100% (176/176), done.\u001b[K\n",
            "remote: Compressing objects: 100% (170/170), done.\u001b[K\n",
            "remote: Total 554 (delta 114), reused 13 (delta 5), pack-reused 378\u001b[K\n",
            "Receiving objects: 100% (554/554), 1.26 MiB | 3.24 MiB/s, done.\n",
            "Resolving deltas: 100% (295/295), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuvlVxwSP1VN",
        "outputId": "9ebafeca-ea94-42b0-ede7-3379f83d0cd0"
      },
      "source": [
        "%cd /content/gpt2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gpt2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-0amY4AP3IV",
        "outputId": "6317ae6b-c486-45be-8726-f2dc7d5828d8"
      },
      "source": [
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (4.41.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (2019.12.20)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (0.4.0)\n",
            "Requirement already satisfied: python-telegram-bot in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (13.5)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (1.1.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire->-r requirements.txt (line 3)) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from fire->-r requirements.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: APScheduler==3.6.3 in /usr/local/lib/python3.7/dist-packages (from python-telegram-bot->-r requirements.txt (line 4)) (3.6.3)\n",
            "Requirement already satisfied: pytz>=2018.6 in /usr/local/lib/python3.7/dist-packages (from python-telegram-bot->-r requirements.txt (line 4)) (2018.9)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from python-telegram-bot->-r requirements.txt (line 4)) (2020.12.5)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/dist-packages (from python-telegram-bot->-r requirements.txt (line 4)) (5.1.1)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->-r requirements.txt (line 5)) (2.0.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->-r requirements.txt (line 5)) (8.0.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->-r requirements.txt (line 5)) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->-r requirements.txt (line 5)) (2.11.3)\n",
            "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.7/dist-packages (from APScheduler==3.6.3->python-telegram-bot->-r requirements.txt (line 4)) (1.5.1)\n",
            "Requirement already satisfied: setuptools>=0.7 in /usr/local/lib/python3.7/dist-packages (from APScheduler==3.6.3->python-telegram-bot->-r requirements.txt (line 4)) (56.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->flask->-r requirements.txt (line 5)) (2.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "P8GRYA0FQLAq",
        "outputId": "ceffe492-2d4a-48f9-e53d-2ba68c6f1d23"
      },
      "source": [
        "!pip3 install tensorflow-gpu==1.15.5"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.15.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/b5/adc281ce4e631251c749d342793795832026edf9035df81c3813ef33fad2/tensorflow_gpu-1.15.5-cp37-cp37m-manylinux2010_x86_64.whl (411.0MB)\n",
            "\u001b[K     |████████████████████████████████| 411.0MB 30kB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 41.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.5) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.5) (3.12.4)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.5) (1.1.2)\n",
            "Requirement already satisfied: h5py<=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.5) (2.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.5) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.5) (0.36.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.5) (1.12.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.5) (3.3.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.5) (1.32.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.5) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.5) (0.12.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 14.3MB/s \n",
            "\u001b[?25hCollecting numpy<1.19.0,>=1.16.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/c6/58e517e8b1fb192725cfa23c01c2e60e4e6699314ee9684a1c5f5c9b27e1/numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.5) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.15.5) (56.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.5) (2.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.5) (3.3.4)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.5) (4.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.5) (3.7.4.3)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=610e03f5ce9cf466ed4c448d5388143e7263df7ca76c05b4b395a701ca76fc8f\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement numpy~=1.19.2, but you'll have numpy 1.18.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement tensorboard~=2.4, but you'll have tensorboard 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement tensorflow-estimator<2.5.0,>=2.4.0, but you'll have tensorflow-estimator 1.15.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, gast, numpy, keras-applications, tensorboard, tensorflow-gpu\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 numpy-1.18.5 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3cQ85NuQkm1",
        "outputId": "b432742c-5a24-4da4-e583-8f9e3e8e5e62"
      },
      "source": [
        "%cd /content/gpt2\n",
        "!python3 download_model.py 774M"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gpt2\n",
            "Fetching checkpoint: 1.00kit [00:00, 743kit/s]                                                      \n",
            "Fetching encoder.json: 1.04Mit [00:01, 975kit/s]                                                    \n",
            "Fetching hparams.json: 1.00kit [00:00, 802kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 3.10Git [11:48, 4.37Mit/s]                                 \n",
            "Fetching model.ckpt.index: 16.0kit [00:00, 9.19Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 1.38Mit [00:01, 1.11Mit/s]                                                \n",
            "Fetching vocab.bpe: 457kit [00:00, 496kit/s]                                                        \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKOesae7Q4hF"
      },
      "source": [
        "!sed -i -e 's/BOTKEYBOTKEYBOTKEYBOTKEYBOTKEY/1827396499:AAHifc06oS31oQ9L3TuCiZxD9EIfKPi0oWQ/' src/GPT2-Learning.py\n",
        "!sed -i -e 's/1558M/774M/' src/GPT2-Learning.py"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQr0Fh6ERmtT",
        "outputId": "ba4642bf-d5dd-42a6-b93b-d02c220052bd"
      },
      "source": [
        "!cat src/GPT2-Learning.py"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#!/usr/bin/env python3\n",
            "# -*- coding: utf-8 -*-\n",
            "from telegram.ext import Updater, CommandHandler, MessageHandler, Filters\n",
            "import fire, json, os, string, sys, threading, random, model, sample, encoder, logging, time\n",
            "import numpy as np\n",
            "import tensorflow as tf\n",
            "import re\n",
            "# Enable logging\n",
            "logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
            "                    level=logging.INFO)\n",
            "logger = logging.getLogger(__name__)\n",
            "\n",
            "# Console output debug prints\n",
            "debug = True\n",
            "# Session timeout\n",
            "timstart = 1500\n",
            "# Model thinking per word 0.66 or 0.77 work well. \n",
            "top = 0.66\n",
            "# Temperature (refer to gpt-2 documentation)\n",
            "temp = 1\n",
            "# top_p multiplier - add to top_p per word \n",
            "# 0.00375‬ - may be shorter\n",
            "# 0.00400\n",
            "# 0.00425\n",
            "# 0.00450\n",
            "# 0.00475\n",
            "# 0.00500 - may be longer\n",
            "mx = 0.00375\n",
            "# This is the start of the learning context.\n",
            "learning = \"\"\n",
            "\n",
            "# End settings\n",
            "mode = False\n",
            "learn = False\n",
            "user = \"\"\n",
            "cache = \"\"\n",
            "running = False\n",
            "temps = str(temp)\n",
            "tpstring = str(top)\n",
            "\n",
            "# Define a few command handlers. These usually take the two arguments bot and\n",
            "# update. Error handlers also receive the raised TelegramError object in error.\n",
            "\n",
            "def start(bot, update):\n",
            "    \"\"\"Send a message when the command /start is issued.\"\"\"\n",
            "    global running\n",
            "    global mode\n",
            "    global learn\n",
            "    global user\n",
            "    global tim\n",
            "    global learning\n",
            "    global cache\n",
            "    if user == \"\":\n",
            "        user = update.message.from_user.id\n",
            "        mode = True\n",
            "        learn = True\n",
            "        learning = \"\"\n",
            "        cache = \"\"\n",
            "        if mode == True and learn == True:\n",
            "            update.message.reply_text('Send a message! Get it computed! 774M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M. I am in the learning chatbot mode.')\n",
            "        if mode == True and learn == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 774M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the chatbot mode.')\n",
            "        if mode == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 774M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the finishsentence mode.')\n",
            "        return\n",
            "    if user == update.message.from_user.id:\n",
            "        mode = True\n",
            "        learn = True\n",
            "        learning = \"\"\n",
            "        cache = \"\"\n",
            "        if mode == True and learn == True:\n",
            "            update.message.reply_text('Send a message! Get it computed! 774M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M. I am in the learning chatbot mode.')\n",
            "        if mode == True and learn == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 774M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the chatbot mode.')\n",
            "        if mode == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 774M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the finishsentence mode.')\n",
            "        return\n",
            "    else:\n",
            "        left = str(tim)\n",
            "        update.message.reply_text('Bot is currently in use, make sure to set your settings when their timer runs down. ' + left + ' seconds.')\n",
            "\n",
            "def help(bot, update):\n",
            "    \"\"\"Send a message when the command /help is issued.\"\"\"\n",
            "    update.message.reply_text('Just type a message... It could be lagged out. /chatbot goes into Me: You: mode. /finish just finishes the text /learnon for conversation learning mode.')\n",
            "\n",
            "def chatbot(bot, update):\n",
            "    \"\"\"Send a message when the command /chatbot is issued.\"\"\"\n",
            "    global running\n",
            "    global mode\n",
            "    global learn\n",
            "    global user\n",
            "    global tim\n",
            "    global learning\n",
            "    global cache\n",
            "    if user == \"\":\n",
            "        user = update.message.from_user.id\n",
            "        mode = True\n",
            "        learn = False\n",
            "        learning = \"\"\n",
            "        cache = \"\"\n",
            "        if mode == True and learn == True:\n",
            "            update.message.reply_text('Send a message! Get it computed! 774M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M. I am in the learning chatbot mode.')\n",
            "        if mode == True and learn == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 774M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the chatbot mode.')\n",
            "        if mode == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 774M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the finishsentence mode.')\n",
            "        return\n",
            "    if user == update.message.from_user.id:\n",
            "        mode = True\n",
            "        learn = False\n",
            "        learning = \"\"\n",
            "        cache = \"\"\n",
            "        if mode == True and learn == True:\n",
            "            update.message.reply_text('Send a message! Get it computed! 774M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M. I am in the learning chatbot mode.')\n",
            "        if mode == True and learn == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 774M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the chatbot mode.')\n",
            "        if mode == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 774M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the finishsentence mode.')\n",
            "        return\n",
            "    else:\n",
            "        left = str(tim)\n",
            "        update.message.reply_text('Bot is currently in use, make sure to set your settings when their timer runs down. ' + left + ' seconds.')\n",
            "\n",
            "def finish(bot, update):\n",
            "    \"\"\"Send a message when the command /finish is issued.\"\"\"\n",
            "    global running\n",
            "    global mode\n",
            "    global learn\n",
            "    global user\n",
            "    global tim\n",
            "    global learning\n",
            "    global cache\n",
            "    if user == \"\":\n",
            "        user = update.message.from_user.id\n",
            "        mode = False\n",
            "        learn = False\n",
            "        learning = \"\"\n",
            "        cache = \"\"\n",
            "        if mode == True and learn == True:\n",
            "            update.message.reply_text('Send a message! Get it computed! 774M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M. I am in the learning chatbot mode.')\n",
            "        if mode == True and learn == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 774M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the chatbot mode.')\n",
            "        if mode == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 774M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the finishsentence mode.')\n",
            "        return\n",
            "    if user == update.message.from_user.id:\n",
            "        mode = False\n",
            "        learn = False\n",
            "        learning = \"\"\n",
            "        cache = \"\"\n",
            "        if mode == True and learn == True:\n",
            "            update.message.reply_text('Send a message! Get it computed! 774M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M. I am in the learning chatbot mode.')\n",
            "        if mode == True and learn == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 774M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the chatbot mode.')\n",
            "        if mode == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 774M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the finishsentence mode.')\n",
            "        return\n",
            "    else:\n",
            "        left = str(tim)\n",
            "        update.message.reply_text('Bot is currently in use, make sure to set your settings when their timer runs down. ' + left + ' seconds.')\n",
            "\n",
            "def learnon(bot, update):\n",
            "    \"\"\"Send a message when the command /learnon is issued.\"\"\"\n",
            "    global running\n",
            "    global mode\n",
            "    global learn\n",
            "    global user\n",
            "    global tim\n",
            "    global learning\n",
            "    global cache\n",
            "    if user == \"\":\n",
            "        user = update.message.from_user.id\n",
            "        mode = True\n",
            "        learn = True\n",
            "        learning = \"\"\n",
            "        cache = \"\"\n",
            "        if mode == True and learn == True:\n",
            "            update.message.reply_text('Send a message! Get it computed! 774M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M. I am in the learning chatbot mode.')\n",
            "        if mode == True and learn == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 774M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the chatbot mode.')\n",
            "        if mode == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 774M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the finishsentence mode.')\n",
            "        return\n",
            "    if user == update.message.from_user.id:\n",
            "        mode = True\n",
            "        learn = True\n",
            "        learning = \"\"\n",
            "        cache = \"\"\n",
            "        if mode == True and learn == True:\n",
            "            update.message.reply_text('Send a message! Get it computed! 774M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M. I am in the learning chatbot mode.')\n",
            "        if mode == True and learn == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 774M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the chatbot mode.')\n",
            "        if mode == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 774M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the finishsentence mode.')\n",
            "        return\n",
            "    else:\n",
            "        left = str(tim)\n",
            "        update.message.reply_text('Bot is currently in use, make sure to set your settings when their timer runs down. ' + left + ' seconds.')\n",
            "\n",
            "def learnoff(bot, update):\n",
            "    \"\"\"Send a message when the command /learnoff is issued.\"\"\"\n",
            "    global running\n",
            "    global mode\n",
            "    global learn\n",
            "    global user\n",
            "    global tim\n",
            "    global learning\n",
            "    global cache\n",
            "    if user == \"\":\n",
            "        user = update.message.from_user.id\n",
            "        mode = True\n",
            "        learn = False\n",
            "        learning = \"\"\n",
            "        cache = \"\"\n",
            "        if mode == True and learn == True:\n",
            "            update.message.reply_text('Send a message! Get it computed! 774M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M. I am in the learning chatbot mode.')\n",
            "        if mode == True and learn == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 774M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the chatbot mode.')\n",
            "        if mode == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 774M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the finishsentence mode.')\n",
            "        return\n",
            "    if user == update.message.from_user.id:\n",
            "        mode = True\n",
            "        learn = False\n",
            "        learning = \"\"\n",
            "        cache = \"\"\n",
            "        if mode == True and learn == True:\n",
            "            update.message.reply_text('Send a message! Get it computed! 774M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M. I am in the learning chatbot mode.')\n",
            "        if mode == True and learn == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 774M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the chatbot mode.')\n",
            "        if mode == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 774M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the finishsentence mode.')\n",
            "        return\n",
            "    else:\n",
            "        left = str(tim)\n",
            "        update.message.reply_text('Bot is currently in use, make sure to set your settings when their timer runs down. ' + left + ' seconds.')\n",
            "\n",
            "def learnreset(bot, update):\n",
            "    \"\"\"Send a message when the command /learnreset is issued.\"\"\"\n",
            "    global running\n",
            "    global mode\n",
            "    global learn\n",
            "    global user\n",
            "    global tim\n",
            "    global learning\n",
            "    global cache\n",
            "    if user == \"\":\n",
            "        user = update.message.from_user.id\n",
            "        mode = True\n",
            "        learn = True\n",
            "        learning = \"\"\n",
            "        cache = \"\"\n",
            "        if mode == True and learn == True:\n",
            "            update.message.reply_text('Send a message! Get it computed! 774M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M. I am in the learning chatbot mode.')\n",
            "        if mode == True and learn == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 774M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the chatbot mode.')\n",
            "        if mode == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 774M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the finishsentence mode.')\n",
            "        return\n",
            "    if user == update.message.from_user.id:\n",
            "        mode = True\n",
            "        learn = True\n",
            "        learning = \"\"\n",
            "        cache = \"\"\n",
            "        if mode == True and learn == True:\n",
            "            update.message.reply_text('Send a message! Get it computed! 774M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M. I am in the learning chatbot mode.')\n",
            "        if mode == True and learn == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 774M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the chatbot mode.')\n",
            "        if mode == False:\n",
            "            update.message.reply_text('Send a message! Get it computed! 774M Settings: Logic: ' + tpstring + ' Rate:' + temps + ' GPT-2 1558M I am in the finishsentence mode.')\n",
            "        return\n",
            "    else:\n",
            "        left = str(tim)\n",
            "        update.message.reply_text('Bot is currently in use, make sure to set your settings when their timer runs down. ' + left + ' seconds.')\n",
            "\n",
            "def regex(mew):\n",
            "    meow = mew\n",
            "    if \"You:\" in meow:\n",
            "        meow = meow[0:meow.find('You:')]\n",
            "        if \"Me:\" in meow:\n",
            "            meow = meow[0:meow.find('Me:')]\n",
            "        return meow\n",
            "    if \"Me:\" in meow:\n",
            "        meow = meow[0:meow.find('Me:')]\n",
            "        if \"You:\" in meow:\n",
            "            meow = meow[0:meow.find('You:')]\n",
            "        return meow\n",
            "    if \"?\" in meow:\n",
            "        meow = meow[0:meow.find('?')]\n",
            "        meow = meow + \"?\"\n",
            "        return meow\n",
            "    if \"!\" in meow:\n",
            "        meow = meow.rsplit('!', 1)[0]\n",
            "        meow = meow + \"!\"\n",
            "        return meow\n",
            "    else:\n",
            "        meow = meow.rsplit('.', 1)[0]\n",
            "        meow = meow + \".\"\n",
            "        return meow\n",
            "    meow = \"Error.\"\n",
            "    return meow\n",
            "\n",
            "\n",
            "def retry(bot, update):\n",
            "    retr = True\n",
            "    top_p = top\n",
            "    temperature = temp\n",
            "    mult = mx\n",
            "    new = retr\n",
            "    comput = threading.Thread(target=wait, args=(bot, update, top_p, temperature, mult, new,))\n",
            "    comput.start()\n",
            "\n",
            "def runn(bot, update):\n",
            "    retr = False\n",
            "    top_p = top\n",
            "    temperature = temp\n",
            "    mult = mx\n",
            "    new = retr\n",
            "    comput = threading.Thread(target=wait, args=(bot, update, top_p, temperature, mult, new,))\n",
            "    comput.start()\n",
            "\n",
            "def wait(bot, update, top_p, temperature, mult, new):\n",
            "    global tim\n",
            "    global user\n",
            "    global running\n",
            "    global mode\n",
            "    global learn\n",
            "    global learning\n",
            "    global cache\n",
            "    if user == \"\":\n",
            "        user = update.message.from_user.id\n",
            "    if user == update.message.from_user.id:\n",
            "        user = update.message.from_user.id\n",
            "        tim = timstart\n",
            "        compute = threading.Thread(target=interact_model, args=(bot, update, top_p, temperature, mult, new,))\n",
            "        compute.start()\n",
            "        if running == False:\n",
            "            while tim > 1:\n",
            "                running = True\n",
            "                time.sleep(1)\n",
            "                tim = tim - 1\n",
            "            if running == True:\n",
            "                mode = False\n",
            "                learn = False\n",
            "                learning = \"\"\n",
            "                cache = \"\"\n",
            "                user = \"\"\n",
            "                update.message.reply_text('Timer has run down, bot has been reset into the default mode.')\n",
            "                running = False\n",
            "    else:\n",
            "        left = str(tim)\n",
            "        update.message.reply_text('Bot is in use, current cooldown is: ' + left + ' seconds.')\n",
            "\n",
            "def interact_model(bot, update, top_p, temperature, mult, new):\n",
            "    model_name = '774M'\n",
            "    seed = random.randint(1431655765, 2863311530)\n",
            "    # random.randint(1, 4294967295)\n",
            "    # random.randint(1073741824, 3221225471)\n",
            "    # random.randint(1431655765, 2863311530)\n",
            "    nsamples = 1\n",
            "    batch_size = 1\n",
            "    top_k = 0\n",
            "    models_dir = 'models'\n",
            "    tex = update.message.text\n",
            "    penguin = str(tex)\n",
            "    global learning\n",
            "    global learn\n",
            "    global mode\n",
            "    global cache\n",
            "#############################################\n",
            "    # This does some basic length processing.\n",
            "    if mode == True:\n",
            "        cat = len(penguin.split())\n",
            "        if cat > 300:\n",
            "            update.message.reply_text('Input text is too long.')\n",
            "            return\n",
            "        if new == True and cache:\n",
            "            m = re.search('.* You: ', cache)\n",
            "            raw_text = m.group(0)\n",
            "            cac = len(raw_text.split())\n",
            "            cat = cac - 2   \n",
            "            length = cat\n",
            "            if cat < 20:\n",
            "                length = 20\n",
            "            if cat > 20:\n",
            "                length = 20\n",
            "            if cat > 30:\n",
            "               length =  40\n",
            "            if cat > 50:\n",
            "                length = 60\n",
            "            if debug == True:\n",
            "                print(\"Cache is...\")\n",
            "                print(raw_text)\n",
            "        if new != True:\n",
            "            wolf = 'Me: ' + penguin\n",
            "            initial = wolf + ' You: '\n",
            "            raw_text = learning + initial\n",
            "            length = cat\n",
            "            if cat < 20:\n",
            "                length = 20\n",
            "            if cat > 20:\n",
            "                length = 20\n",
            "            if cat > 30:\n",
            "               length =  40\n",
            "            if cat > 50:\n",
            "                length = 60\n",
            "            cache = raw_text\n",
            "        tgt = len(raw_text.split())\n",
            "        if tgt > 300:\n",
            "            while tgt > 300:\n",
            "                if debug == True:\n",
            "                    print(\"Reducing memory of chat.\")\n",
            "                raw_text = raw_text.split(' Me:', 1)[-1]\n",
            "                raw_text = \"Me:\" + raw_text\n",
            "                tgt = len(raw_text.split())\n",
            "                if tgt > 300:\n",
            "                    if debug == True:\n",
            "                        print(\"Reducing memory of chat.\")\n",
            "                    raw_text = raw_text.split('You:', 1)[-1]\n",
            "                    raw_text = \"You:\" + raw_text\n",
            "                    tgt = len(raw_text.split())\n",
            "            if debug == True:\n",
            "                print(\"FINAL MEMORY REDUCTION:\")\n",
            "                print(raw_text)\n",
            "    if mode == False:\n",
            "        cat = len(penguin.split())\n",
            "        length = cat\n",
            "        if length > 300:\n",
            "            update.message.reply_text('Input text is too long.')\n",
            "            return\n",
            "        if new != True:\n",
            "            cache = penguin\n",
            "        if new == True and cache:\n",
            "            penguin = cache\n",
            "            length = len(penguin.split())\n",
            "            cat = length\n",
            "            if debug == True:\n",
            "                print(\"Cache is...\")\n",
            "                print(penguin)\n",
            "        raw_text = penguin\n",
            "    tx = float(top_p)\n",
            "    cax = float(cat)\n",
            "    cay = float(mx)\n",
            "    caz = float(cax * cay)\n",
            "    #ta = ((1-tx)/caz)\n",
            "    #top_p = ((tx) + (ta))\n",
            "    top_p = caz + tx\n",
            "    if top_p > 0.84:\n",
            "        top_p = 0.84\n",
            "    if top_p < 0.005:\n",
            "        top_p = 0.005\n",
            "#############################################\n",
            "    update.message.reply_text('Computing...')\n",
            "    models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
            "    if batch_size is None:\n",
            "        batch_size = 1\n",
            "    assert nsamples % batch_size == 0\n",
            "    enc = encoder.get_encoder(model_name, models_dir)\n",
            "    hparams = model.default_hparams()\n",
            "    with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:\n",
            "        hparams.override_from_dict(json.load(f))\n",
            "    if length is None:\n",
            "        length = hparams.n_ctx // 2\n",
            "    elif length > hparams.n_ctx:\n",
            "        raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
            "    with tf.Session(graph=tf.Graph()) as sess:\n",
            "        context = tf.placeholder(tf.int32, [batch_size, None])\n",
            "        np.random.seed(seed)\n",
            "        tf.set_random_seed(seed)\n",
            "        output = sample.sample_sequence(\n",
            "            hparams=hparams, length=length,\n",
            "            context=context,\n",
            "            batch_size=batch_size,\n",
            "            temperature=temperature, top_k=top_k, top_p=top_p\n",
            "        )\n",
            "        saver = tf.train.Saver()\n",
            "        ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n",
            "        saver.restore(sess, ckpt)\n",
            "        context_tokens = enc.encode(raw_text)\n",
            "        generated = 0\n",
            "        for _ in range(nsamples // batch_size):\n",
            "            out = sess.run(output, feed_dict={\n",
            "                context: [context_tokens for _ in range(batch_size)]\n",
            "            })[:, len(context_tokens):]\n",
            "            for i in range(batch_size):\n",
            "                generated += 1\n",
            "                text = enc.decode(out[i])\n",
            "                if debug == True:\n",
            "                    print(\"==========\")\n",
            "                    print(\"Before splitlines: \" + text)\n",
            "                    print(\"==========\")\n",
            "                if mode == True:\n",
            "                    pika = text.splitlines()[0]\n",
            "                else:\n",
            "                    pika = text\n",
            "                stripes = pika.encode(encoding=sys.stdout.encoding,errors='ignore')\n",
            "                tigger = stripes.decode(\"utf-8\")\n",
            "                mew = str(tigger)\n",
            "                # disable any regex on finishsentence mode.\n",
            "                if mode == True:\n",
            "                    meo = regex(mew)\n",
            "                    meow = \" \".join(re.split(\"[^a-zA-Z.,?!'*]*\", meo))\n",
            "                    # Final regex\n",
            "                else:\n",
            "                    meow = mew\n",
            "                if learn == True:\n",
            "                    learning = raw_text + meow + \" \"\n",
            "                update.message.reply_text(meow)\n",
            "                if debug == True:\n",
            "                    print(\"==========\")\n",
            "                    mod = str(mode)\n",
            "                    print(\"Mode: \" + mod)\n",
            "                    lear = str(learn)\n",
            "                    print(\"Learn: \" + lear)\n",
            "                    lent = str(length)\n",
            "                    print(\"Length: \" + lent)\n",
            "                    print(\"==========\")\n",
            "                    ball = str(pika)\n",
            "                    print(\"Before regex: \" + ball)\n",
            "                    print(\"==========\")\n",
            "                    print(\"Output: \" + meow)\n",
            "                    print(\"==========\")\n",
            "                    print(\"Raw_text or Original: \" + raw_text)\n",
            "                    print(\"==========\")\n",
            "                    print(\"Learning text or Next: \" + learning)\n",
            "                    print(\"==========\")\n",
            "                    tps = str(top_p)\n",
            "                    print(\"top_p out: \" + tps)\n",
            "                    print(\"==========\")\n",
            "                    tpa = str(tx)\n",
            "                    print(\"top_p in: \" + tpa)\n",
            "                    print(\"==========\")\n",
            "    sess.close()\n",
            "\n",
            "def error(bot, update):\n",
            "    \"\"\"Log Errors caused by Updates.\"\"\"\n",
            "    logger.warning('Update \"%s\" caused error \"%s\"', update)\n",
            "\n",
            "def main():\n",
            "    \"\"\"Start the bot.\"\"\"\n",
            "    # Create the Updater and pass it your bot's token.\n",
            "    # Make sure to set use_context=True to use the new context based callbacks\n",
            "    # Post version 12 this will no longer be necessary\n",
            "    updater = Updater(\"1827396499:AAHifc06oS31oQ9L3TuCiZxD9EIfKPi0oWQ\", use_context=False)\n",
            "    # Get the dispatcher to register handlers\n",
            "    dp = updater.dispatcher\n",
            "    # on different commands - answer in Telegram\n",
            "    dp.add_handler(CommandHandler(\"start\", start))\n",
            "    dp.add_handler(CommandHandler(\"help\", help))\n",
            "    dp.add_handler(CommandHandler(\"chatbot\", chatbot))\n",
            "    dp.add_handler(CommandHandler(\"finish\", finish))\n",
            "    dp.add_handler(CommandHandler(\"learnon\", learnon))\n",
            "    dp.add_handler(CommandHandler(\"learnoff\", learnoff))\n",
            "    dp.add_handler(CommandHandler(\"learnreset\", learnreset))\n",
            "    dp.add_handler(CommandHandler(\"retry\", retry))\n",
            "    # on noncommand i.e message - echo the message on Telegram\n",
            "    dp.add_handler(MessageHandler(Filters.text, runn))\n",
            "    # log all errors\n",
            "    dp.add_error_handler(error)\n",
            "    # Start the Bot\n",
            "    updater.start_polling()\n",
            "    # Run the bot until you press Ctrl-C or the process receives SIGINT,\n",
            "    # SIGTERM or SIGABRT. This should be used most of the time, since\n",
            "    # start_polling() is non-blocking and will stop the bot gracefully.\n",
            "    updater.idle()\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    fire.Fire(main)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3vFOOX8TANZ",
        "outputId": "7df044ea-f7f7-4ca4-818d-ad3a05570ad8"
      },
      "source": [
        "!./start"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src/GPT2-Learning.py:544: TelegramDeprecationWarning: Old Handler API is deprecated - see https://git.io/fxJuV for details\n",
            "  updater = Updater(\"1827396499:AAHifc06oS31oQ9L3TuCiZxD9EIfKPi0oWQ\", use_context=False)\n",
            "2021-05-19 06:40:03,180 - apscheduler.scheduler - INFO - Scheduler started\n",
            "WARNING:tensorflow:From src/GPT2-Learning.py:466: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2021-05-19 06:40:15,001 - tensorflow - WARNING - From src/GPT2-Learning.py:466: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2021-05-19 06:40:15.125518: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2021-05-19 06:40:15.216863: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-05-19 06:40:15.216961: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (557e3809c343): /proc/driver/nvidia/version does not exist\n",
            "2021-05-19 06:40:15.221429: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2021-05-19 06:40:15.294678: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2021-05-19 06:40:15.295785: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558d86dd5d40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2021-05-19 06:40:15.295826: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "WARNING:tensorflow:From src/GPT2-Learning.py:467: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "2021-05-19 06:40:15,301 - tensorflow - WARNING - From src/GPT2-Learning.py:467: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From src/GPT2-Learning.py:469: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "2021-05-19 06:40:15,307 - tensorflow - WARNING - From src/GPT2-Learning.py:469: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt2/src/sample.py:51: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "2021-05-19 06:40:15,307 - tensorflow - WARNING - From /content/gpt2/src/sample.py:51: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "2021-05-19 06:40:15,308 - tensorflow - WARNING - From /content/gpt2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "2021-05-19 06:40:15,330 - tensorflow - WARNING - From /content/gpt2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "2021-05-19 06:40:15,371 - tensorflow - WARNING - From /content/gpt2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt2/src/sample.py:64: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "2021-05-19 06:40:22,014 - tensorflow - WARNING - From /content/gpt2/src/sample.py:64: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt2/src/sample.py:39: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "2021-05-19 06:40:22,084 - tensorflow - WARNING - From /content/gpt2/src/sample.py:39: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt2/src/sample.py:67: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "2021-05-19 06:40:22,084 - tensorflow - WARNING - From /content/gpt2/src/sample.py:67: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From src/GPT2-Learning.py:476: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "2021-05-19 06:40:28,063 - tensorflow - WARNING - From src/GPT2-Learning.py:476: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "INFO:tensorflow:Restoring parameters from models/774M/model.ckpt\n",
            "2021-05-19 06:40:28,573 - tensorflow - INFO - Restoring parameters from models/774M/model.ckpt\n",
            "==========\n",
            "Before splitlines: ಠ_ಠ I'm good. Do you have any free time? Me:\n",
            "==========\n",
            "==========\n",
            "Mode: True\n",
            "Learn: True\n",
            "Length: 20\n",
            "==========\n",
            "Before regex: ಠ_ಠ I'm good. Do you have any free time? Me:\n",
            "==========\n",
            "Output:   I ' m  g o o d .  D o  y o u  h a v e  a n y  f r e e  t i m e ?  \n",
            "==========\n",
            "Raw_text or Original: Me: Hello. How are you? You: \n",
            "==========\n",
            "Learning text or Next: Me: Hello. How are you? You:   I ' m  g o o d .  D o  y o u  h a v e  a n y  f r e e  t i m e ?   \n",
            "==========\n",
            "top_p out: 0.675\n",
            "==========\n",
            "top_p in: 0.66\n",
            "==========\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}